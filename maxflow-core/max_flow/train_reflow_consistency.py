"""
Reflow Distillation Trainer with Consistency Distillation (Phase 25)
Fine-tunes a student model to perform single-step generation with consistency.

Method:
1. Load dataset of pairs (x_0, x_1) generated by Teacher.
2. Objective: Minimize MSE(v_pred(t, x_t), x_1 - x_0) + Consistency Loss.
3. The velocity field learns to point straight from noise to data.
4. Consistency distillation ensures the student matches teacher's behavior.

Reference: Rectified Flow (ICLR 2023), Instaflow (ICLR 2024), Consistency Models.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import os
import json
from tqdm import tqdm
from torch_geometric.loader import DataLoader
from max_flow.models.max_rl import MaxFlow
from max_flow.utils.quality_assessment import calculate_consistency
from accelerate import Accelerator


class ReflowPairDataset(torch.utils.data.Dataset):
    def __init__(self, data_path):
        print(f"Loading Reflow dataset from {data_path}...")
        # Load to CPU first, Accelerator handles device placement
        self.data = torch.load(data_path, map_location='cpu')
        print(f"Loaded {len(self.data)} pairs.")
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        return self.data[idx]


def train_reflow(
    dataset_path,
    checkpoint_path,
    save_dir,
    epochs=10,
    batch_size=32,
    lr=1e-5,
    consistency_weight=0.1,
    temperature=0.1,
    teacher_checkpoint=None
):
    """
    Train Reflow model with Consistency Distillation
    
    Args:
        dataset_path: Path to reflow dataset
        checkpoint_path: Path to initial student model
        save_dir: Directory to save trained model
        epochs: Number of training epochs
        batch_size: Batch size
        lr: Learning rate
        consistency_weight: Weight for consistency loss
        temperature: Temperature for consistency distillation
        teacher_checkpoint: Path to teacher model for consistency distillation
    """
    # Mixed Precision: SOTA preference for bfloat16 if T4/A100/H100
    mixed_precision = "fp16"
    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
        mixed_precision = "bf16"

    accelerator = Accelerator(mixed_precision=mixed_precision)
    device = accelerator.device
    os.makedirs(save_dir, exist_ok=True)
    
    accelerator.print(f"üöÄ Reflow Distillation (Consistency) on {device} | Precision: {mixed_precision}")
    
    # 2. Initialize Student
    accelerator.print("Initializing Student Model...")
    try:
        # SOTA Phase 65: Updated node_in_dim=58
        student = MaxFlow(node_in_dim=58, hidden_dim=args.hidden_dim, num_layers=args.num_layers)
        # Load checkpoint on CPU first
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint.get('model_state_dict', checkpoint.get('state_dict', checkpoint))
        student.load_state_dict(state_dict)
        accelerator.print("‚úÖ Student initialized from Teacher weights")
    except Exception as e:
        accelerator.print(f"‚ö†Ô∏è Could not load checkpoint ({e}). Initializing from scratch.")
        student = MaxFlow(node_in_dim=58, hidden_dim=args.hidden_dim, num_layers=args.num_layers)
        
    if hasattr(torch, "compile"):
        accelerator.print("Compiling student with torch.compile...")
        student.backbone = torch.compile(student.backbone, mode="reduce-overhead")
        
    # 3. Initialize Teacher for consistency distillation (optional)
    teacher = None
    if teacher_checkpoint:
        try:
            teacher = MaxFlow(node_in_dim=58, hidden_dim=args.hidden_dim, num_layers=args.num_layers)
            teacher_checkpoint_data = torch.load(teacher_checkpoint, map_location='cpu')
            teacher_state_dict = teacher_checkpoint_data.get('model_state_dict', teacher_checkpoint_data.get('state_dict', teacher_checkpoint_data))
            teacher.load_state_dict(teacher_state_dict)
            teacher.eval()
            accelerator.print("‚úÖ Teacher model loaded for consistency distillation")
        except Exception as e:
            accelerator.print(f"‚ö†Ô∏è Could not load teacher checkpoint ({e}). Consistency distillation disabled.")
    
    optimizer = torch.optim.AdamW(student.parameters(), lr=lr)
    
    # 4. Data Loader
    dataset = ReflowPairDataset(dataset_path)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # 5. Prepare with Accelerator
    student, optimizer, loader = accelerator.prepare(student, optimizer, loader)
    if teacher:
        teacher = accelerator.prepare(teacher)
    
    # 6. Training Loop
    student.train()
    for epoch in range(epochs):
        total_loss = 0
        total_mse_loss = 0
        total_consistency_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{epochs}", disable=not accelerator.is_local_main_process)
        
        for batch in pbar:
            # Accelerator handles batch device placement
            x_0 = batch.pos_L
            x_1 = batch.target_pos
            
            optimizer.zero_grad()
            
            # Sample t uniformly U[0, 1] for each graph
            num_graphs = batch.num_graphs
            t = torch.rand(num_graphs, device=device)
            
            # Broadcast t to nodes
            batch_idx = getattr(batch, 'x_L_batch', getattr(batch, 'batch', None))
            if batch_idx is not None:
                t_nodes = t[batch_idx].unsqueeze(-1)
            else:
                t_nodes = t.view(-1, 1).expand(x_0.size(0), -1)
            
            # Interpolate x_t
            x_t = t_nodes * x_1 + (1 - t_nodes) * x_0
            batch.pos_L = x_t
            
            # Forward pass through student
            with torch.no_grad() if teacher else None:
                v_pred = student.flow.model(batch, t_nodes).pos
                
            # Calculate target velocity
            target_velocity = (x_1 - x_0) / t_nodes.clamp_min(1e-8)
            
            # MSE Loss
            mse_loss = torch.nn.functional.mse_loss(v_pred, target_velocity)
            
            # Consistency Distillation Loss
            consistency_loss = torch.tensor(0.0, device=device)
            if teacher:
                with torch.no_grad():
                    v_teacher = teacher.flow.model(batch, t_nodes).pos
                
                # KL divergence between student and teacher velocities
                consistency_loss = torch.nn.functional.kl_div(
                    torch.log_softmax(v_pred / temperature, dim=-1),
                    torch.softmax(v_teacher / temperature, dim=-1),
                    reduction='batchmean'
                ) * (temperature ** 2)
            
            # Total loss
            loss = mse_loss + consistency_weight * consistency_loss
            
            # Backward pass
            accelerator.backward(loss)
            optimizer.step()
            
            # Update metrics
            total_loss += loss.item()
            total_mse_loss += mse_loss.item()
            total_consistency_loss += consistency_loss.item()
            
            # Progress update
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'MSE': f"{mse_loss.item():.4f}",
                'Consistency': f"{consistency_loss.item():.4f}"
            })
            
        # Epoch summary
        if accelerator.is_main_process:
            print(f"üìä Epoch {epoch+1} Summary:")
            print(f"üìà Avg Loss: {total_loss / len(loader):.4f}")
            print(f"üìà Avg MSE Loss: {total_mse_loss / len(loader):.4f}")
            print(f"üìà Avg Consistency Loss: {total_consistency_loss / len(loader):.4f}")
            
            # Save checkpoint
            if (epoch + 1) % 5 == 0 or epoch + 1 == epochs:
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': student.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': total_loss / len(loader)
                }
                torch.save(checkpoint, os.path.join(save_dir, f'reflow_epoch_{epoch+1}.pt'))
                print(f"üíæ Checkpoint saved to {save_dir}/reflow_epoch_{epoch+1}.pt")
    
    # Final checkpoint
    checkpoint = {
        'epoch': epochs,
        'model_state_dict': student.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(loader)
    }
    torch.save(checkpoint, os.path.join(save_dir, 'reflow_final.pt'))
    print(f"üéâ Training complete! Final model saved to {save_dir}/reflow_final.pt")
    
    return student


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Reflow Distillation Trainer with Consistency")
    parser.add_argument("--dataset", type=str, required=True, help="Path to reflow dataset")
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to initial student model")
    parser.add_argument("--save_dir", type=str, required=True, help="Directory to save trained model")
    parser.add_argument("--epochs", type=int, default=10, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size")
    parser.add_argument("--lr", type=float, default=1e-5, help="Learning rate")
    parser.add_argument("--consistency_weight", type=float, default=0.1, help="Weight for consistency loss")
    parser.add_argument("--temperature", type=float, default=0.1, help="Temperature for consistency distillation")
    parser.add_argument("--teacher_checkpoint", type=str, default=None, help="Path to teacher model for consistency distillation")
    parser.add_argument("--hidden_dim", type=int, default=128)
    parser.add_argument("--num_layers", type=int, default=4)
    
    args = parser.parse_args()
    
    train_reflow(
        args.dataset,
        args.checkpoint,
        args.save_dir,
        args.epochs,
        args.batch_size,
        args.lr,
        args.consistency_weight,
        args.temperature,
        args.teacher_checkpoint
    )
